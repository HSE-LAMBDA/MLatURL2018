{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# run those lines if you haven't installed gym yet.\n",
        "# !pip install --upgrade gym >> install.log\n",
        "\n",
        "# the lines below is only needed if you run on an everware server\n",
        "# !apt-get -qq update \n",
        "# !apt-get install -y xvfb >> install.log\n",
        "# !apt-get install -y ffmpeg >> install.log"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#XVFB will be launched if you run on a server\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ./xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice: \"deep\" crossentropy method with neural nets\n",
        "This notebook will teach you to solve reinforcement learning problems with crossentropy method with neural network policy.\n",
        "\n",
        "![img](https://casd35.wikispaces.com/file/view/digging_deeper_final.jpg/359658499/503x260/digging_deeper_final.jpg)\n",
        "\nIn this section we will train a neural network policy for continuous state space game"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"CartPole-v0\").env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "\nplt.imshow(env.render(\"rgb_array\"))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#create agent\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "agent = MLPClassifier(hidden_layer_sizes=(20,20),\n",
        "                      activation='tanh',\n",
        "                      warm_start=True, #keep progress between .fit(...) calls\n",
        "                      max_iter=1 #make only 1 iteration on each .fit(...)\n",
        "                     )\n",
        "#initialize agent to the dimension of state an amount of actions\n",
        "agent.fit([env.reset()]*n_actions,range(n_actions));\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating sessions\n",
        "\nIn the next section we can use agent's predict_proba method to sample actions."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_session(t_max=1000):\n",
        "    \n",
        "    states,actions = [],[]\n",
        "    total_reward = 0\n",
        "    \n",
        "    s = env.reset()\n",
        "    \n",
        "    for t in range(t_max):\n",
        "        \n",
        "        #predict array of action probabilities\n",
        "        probs = agent.predict_proba([s])[0] \n",
        "        \n",
        "        a = <sample action with such probabilities>\n",
        "        \n",
        "        new_s,r,done,info = env.step(a)\n",
        "        \n",
        "        #record sessions like you did before\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        total_reward+=r\n",
        "        \n",
        "        s = new_s\n",
        "        if done: break\n",
        "    return states,actions,total_reward\n",
        "        "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "s,a,r = generate_session()\n",
        "assert type(s) == type(a) == list\n",
        "assert len(s) == len(a)\n",
        "assert type(r) in [float,np.float]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#record sessions\n",
        "import gym.wrappers\n",
        "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
        "sessions = [generate_session() for _ in range(10)]\n",
        "env.close()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#show video\n",
        "from IPython.display import HTML\n",
        "import os\n",
        "\n",
        "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#let's see the initial reward distribution\n",
        "sample_rewards = [generate_session()[-1] for _ in range(200)]\n",
        "\n",
        "plt.hist(sample_rewards,bins=20);\n",
        "plt.vlines([np.percentile(sample_rewards,50)],[0],[100],label=\"50'th percentile\",color='green')\n",
        "plt.vlines([np.percentile(sample_rewards,90)],[0],[100],label=\"90'th percentile\",color='red')\n",
        "plt.legend()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crossentropy method steps (2pts)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def select_elites(states_batch,actions_batch,rewards_batch,percentile=50):\n",
        "    \"\"\"\n",
        "    Select states and actions from games that have rewards >= percentile\n",
        "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
        "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
        "    :param rewards_batch: list of rewards, rewards_batch[session_i][t]\n",
        "    \n",
        "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
        "    \n",
        "    Please return elite states and actions in their original order \n",
        "    [i.e. sorted by session number and timestep within session]\n",
        "    \n",
        "    If you're confused, see examples below. Please don't assume that states are integers (they'll get different later).\n",
        "    \"\"\"\n",
        "    \n",
        "    reward_threshold = np.percentile(rewards_batch,percentile)\n",
        "    \n",
        "    elite_states  = <your code here>\n",
        "    elite_actions = <your code here>\n",
        "    \n",
        "    return elite_states,elite_actions\n",
        "    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "states_batch = [\n",
        "    [1,2,3],   #game1\n",
        "    [4,2,0,2], #game2\n",
        "    [3,1]      #game3\n",
        "]\n",
        "\n",
        "actions_batch = [\n",
        "    [0,2,4],   #game1\n",
        "    [3,2,0,1], #game2\n",
        "    [3,3]      #game3\n",
        "]\n",
        "rewards_batch = [\n",
        "    3,         #game1\n",
        "    4,         #game2\n",
        "    5,         #game3\n",
        "]\n",
        "\n",
        "test_result_0 = select_elites(states_batch,actions_batch,rewards_batch,percentile=0)\n",
        "test_result_40 = select_elites(states_batch,actions_batch,rewards_batch,percentile=30)\n",
        "test_result_90 = select_elites(states_batch,actions_batch,rewards_batch,percentile=90)\n",
        "test_result_100 = select_elites(states_batch,actions_batch,rewards_batch,percentile=100)\n",
        "\n",
        "assert np.all(test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1])  \\\n",
        "   and np.all(test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]),\\\n",
        "        \"For percentile 0 you should return all states and actions in chronological order\"\n",
        "assert np.all(test_result_40[0] == [4, 2, 0, 2, 3, 1]) and \\\n",
        "        np.all(test_result_40[1] ==[3, 2, 0, 1, 3, 3]),\\\n",
        "        \"For percentile 30 you should only select states/actions from two first\"\n",
        "assert np.all(test_result_90[0] == [3,1]) and \\\n",
        "        np.all(test_result_90[1] == [3,3]),\\\n",
        "        \"For percentile 90 you should only select states/actions from one game\"\n",
        "assert np.all(test_result_100[0] == [3,1]) and\\\n",
        "       np.all(test_result_100[1] == [3,3]),\\\n",
        "        \"Please make sure you use >=, not >. Also double-check how you compute percentile.\"\n",
        "print(\"Ok!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_policy(elite_states, elite_actions):\n",
        "    \"\"\"\n",
        "    Given old policy and a list of elite states/actions from select_elites,\n",
        "    return new updated policy where each action probability is proportional to\n",
        "    \n",
        "    policy[s_i,a_i] ~ #[occurences of si and ai in elite states/actions]\n",
        "    \n",
        "    Don't forget to normalize policy to get valid probabilities and handle 0/0 case.\n",
        "    In case you never visited a state, set probabilities for all actions to 1./n_actions\n",
        "    \n",
        "    :param elite_states: 1D list of states from elite sessions\n",
        "    :param elite_actions: 1D list of actions from elite sessions\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    new_policy = np.zeros([n_states,n_actions])\n",
        "    \n",
        "    <Your code here: update probabilities for actions given elite states & actions>\n",
        "    #Don't forget to set 1/n_actions for all actions in unvisited states.\n",
        "    \n",
        "    \n",
        "    return new_policy"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "elite_states, elite_actions = ([1, 2, 3, 4, 2, 0, 2, 3, 1], [0, 2, 4, 3, 2, 0, 1, 3, 3])\n",
        "\n\n",
        "new_policy = update_policy(elite_states,elite_actions)\n",
        "\n",
        "assert np.isfinite(new_policy).all(), \"Your new policy contains NaNs or +-inf. Make sure you don't divide by zero.\"\n",
        "assert np.all(new_policy>=0), \"Your new policy can't have negative action probabilities\"\n",
        "assert np.allclose(new_policy.sum(axis=-1),1), \"Your new policy should be a valid probability distribution over actions\"\n",
        "reference_answer = np.array([\n",
        "       [ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
        "       [ 0.5       ,  0.        ,  0.        ,  0.5       ,  0.        ],\n",
        "       [ 0.        ,  0.33333333,  0.66666667,  0.        ,  0.        ],\n",
        "       [ 0.        ,  0.        ,  0.        ,  0.5       ,  0.5       ]])\n",
        "assert np.allclose(new_policy[:4,:5],reference_answer)\n",
        "print(\"Ok!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop\n",
        "Generate sessions, select N best and fit to those."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def show_progress(rewards_batch,log, reward_range=[-990,+10]):\n",
        "    \"\"\"\n",
        "    A convenience function that displays training progress. \n",
        "    No cool math here, just charts.\n",
        "    \"\"\"\n",
        "    \n",
        "    mean_reward = np.mean(rewards_batch)\n",
        "    threshold = np.percentile(rewards_batch,percentile)\n",
        "    log.append([mean_reward,threshold])\n",
        "\n",
        "    clear_output(True)\n",
        "    print(\"mean reward = %.3f, threshold=%.3f\"%(mean_reward,threshold))\n",
        "    plt.figure(figsize=[8,4])\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(list(zip(*log))[0],label='Mean rewards')\n",
        "    plt.plot(list(zip(*log))[1],label='Reward thresholds')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    \n",
        "    plt.subplot(1,2,2)\n",
        "    plt.hist(rewards_batch,range=reward_range);\n",
        "    plt.vlines([np.percentile(rewards_batch,percentile)],[0],[100],label=\"percentile\",color='red')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main loop\n",
        "\nWe can now combine the functions we wrote to train the agent with crossentropy method."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "n_sessions = 100\n",
        "percentile = 70\n",
        "log = []\n",
        "\n",
        "for i in range(100):\n",
        "    #generate new sessions\n",
        "    sessions = [<generate a list of n_sessions new sessions by calling generate_session several times>]\n",
        "\n",
        "    states_batch,actions_batch,rewards_batch = map(np.array,zip(*sessions))\n",
        "\n",
        "    elite_states, elite_actions = <select elite states/actions for training>\n",
        "    \n",
        "    <fit agent to predict elite_actions(y) from elite_states(X)>\n",
        "\n",
        "    show_progress(rewards_batch,log,reward_range=[0,np.max(rewards_batch)])\n",
        "    \n",
        "    if np.mean(rewards_batch)> 190:\n",
        "        print(\"You Win! You may stop training now via KeyboardInterrupt.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#record sessions\n",
        "import gym.wrappers\n",
        "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
        "sessions = [generate_session() for _ in range(100)]\n",
        "env.close()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#show video\n",
        "from IPython.display import HTML\n",
        "import os\n",
        "\n",
        "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "name": "python",
      "nbconvert_exporter": "python",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "version": "3.5.2",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}